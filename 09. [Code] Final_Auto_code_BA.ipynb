{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kikiru328/Bone_Detection/blob/main/Final_Auto_code_BA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Z5JDACYXP0z"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cg6TRCG7p90K"
      },
      "source": [
        "# Source Code\n",
        "공용폴더에 폴더(yolov5), 파일(model.pt) 업로드해뒀습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWEFnlGhnY_Q"
      },
      "source": [
        "## Preprocessing Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Af0uQ1JdXXMc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import glob\n",
        "import math\n",
        "\n",
        "\n",
        "def read_img(path):\n",
        "    original_img = cv2.imread(path)\n",
        "    return original_img\n",
        "\n",
        "########## Making mask for removing background #############\n",
        "\n",
        "def make_mask(original_img):\n",
        "\n",
        "    ## change to lab for making mask\n",
        "\n",
        "    img_mask = original_img.copy()\n",
        "\n",
        "    img_mask = cv2.cvtColor(img_mask, cv2.COLOR_RGB2BGR)\n",
        "    img_mask = cv2.cvtColor(img_mask, cv2.COLOR_BGR2Lab)\n",
        "    \n",
        "    ## blur _02 \n",
        "    # kernel_size = odds / value = img.mean()\n",
        "\n",
        "\n",
        "    blur_k = int((img_mask.mean()*0.5)//2)*2+1\n",
        "    img_mask = cv2.medianBlur(img_mask, blur_k)\n",
        "    \n",
        "    ## change to Grayscale for threshold\n",
        "\n",
        "    img_mask = cv2.cvtColor(img_mask, cv2.COLOR_Lab2BGR)\n",
        "    img_mask = cv2.cvtColor(img_mask, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    ## binary / value = img.mean()\n",
        "\n",
        "    if img_mask.mean() > 100 : \n",
        "      th = img_mask.mean()*0.94\n",
        "    else : \n",
        "      th = img_mask.mean()\n",
        "\n",
        "    ret, img_mask = cv2.threshold(img_mask, th, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    ## mask based Max value of contours\n",
        "\n",
        "    contours, hierarchy = cv2.findContours(img_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    max_cnt = max(contours, key=cv2.contourArea)\n",
        "    mask = np.zeros(img_mask.shape, dtype=np.uint8)\n",
        "    cv2.drawContours(mask, [max_cnt], -1, (255,255,255), -1)\n",
        "    \n",
        "    ## Applying for dilation\n",
        "\n",
        "    k = cv2.getStructuringElement(cv2.MORPH_RECT, (8,8))\n",
        "    mask = cv2.dilate(mask,k)\n",
        "    return mask\n",
        "\n",
        "######## background cut based mask ##########\n",
        "\n",
        "def cut_mask(original_img, mask):\n",
        "\n",
        "    ## copying\n",
        "    img_for_cut = original_img.copy()\n",
        "\n",
        "    ## H/W\n",
        "    height, width = img_for_cut.shape[:2]\n",
        "\n",
        "    ## mask\n",
        "    mask_list = mask.tolist()\n",
        "    \n",
        "    for y in range(int(height*0.05),height):\n",
        "        if max(mask[y,int(width*0.3):int(width*0.7)]) > 0:\n",
        "            start_y = y-int(height*0.05)\n",
        "            break\n",
        "            \n",
        "    for x in range(int(width*0.05),width):\n",
        "        if max(mask[int(height*0.3):int(height*0.7),x]) > 0:\n",
        "            start_x = x-int(width*0.05)\n",
        "            break\n",
        "            \n",
        "    for x in range(int(width*0.95),-1,-1):\n",
        "        if max(mask[int(height*0.3):int(height*0.7),x]) > 0:\n",
        "            end_x = x+int(width*0.05)\n",
        "            break\n",
        "            \n",
        "    cut_index = 0\n",
        "    if mask_list[height-1][-1] == 255 or mask_list[height-1][0] == 255:\n",
        "        for n in reversed(range(height)):\n",
        "            if mask_list[n][0] == 0 or mask_list[n][-1] == 0:\n",
        "                cut_index = n\n",
        "                break\n",
        "                \n",
        "    if cut_index == 0:\n",
        "        cut_index = height\n",
        "\n",
        "    ## converting color\n",
        "    img_for_cut = cv2.cvtColor(img_for_cut, cv2.COLOR_BGR2GRAY) \n",
        "\n",
        "    img_for_cut = img_for_cut[start_y:(cut_index-1),start_x:end_x]\n",
        "    mask = mask[start_y:(cut_index-1),start_x:end_x]\n",
        "\n",
        "    ## remove background\n",
        "    masked = cv2.bitwise_and(img_for_cut, mask)\n",
        "\n",
        "    return masked\n",
        "\n",
        "\n",
        "######## Overlaying ########\n",
        "\n",
        "def overlay_contours(masked):\n",
        "    copied_img = masked.copy()\n",
        "\n",
        "    ## smoothing\n",
        "    gray_smooth = cv2.bilateralFilter(copied_img, 7,60,60)\n",
        "\n",
        "    ## contours\n",
        "    outline = cv2.Canny(gray_smooth,50,50)\n",
        "\n",
        "    ## overlay\n",
        "    overlaied_img = cv2.add(masked,outline)\n",
        "    return overlaied_img\n",
        "\n",
        "######## Rotation ########\n",
        "\n",
        "def img_rotation(overlaied_img):\n",
        "\n",
        "    ## copying img\n",
        "    before_rot_img = overlaied_img.copy()\n",
        "\n",
        "    \n",
        "    h, w = before_rot_img.shape[:2]\n",
        "    before_rot_img = cv2.cvtColor(before_rot_img, cv2.COLOR_RGB2BGR)\n",
        "    gray = cv2.cvtColor(before_rot_img, cv2.COLOR_BGR2GRAY)\n",
        "    ret, th = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
        "    th_li = th.tolist()\n",
        "\n",
        "    ## Rotation stage 01\n",
        "    # lower = first black spot\n",
        "\n",
        "    for i in reversed(range(h)):\n",
        "        if th_li[i][0] == 0 and th_li[i][-1] == 0:\n",
        "            lower = i\n",
        "            break\n",
        "\n",
        "    # lower = condition ; bottom = lower / img * 0.95\n",
        "\n",
        "    if lower == h - 1:\n",
        "        lower = int(h*0.9)\n",
        "\n",
        "    # upper = condition ; lower + lower * 0.05\n",
        "\n",
        "    slice5 = int(len(th)*0.05)\n",
        "    upper = lower - slice5\n",
        "\n",
        "    # x, y = between upper and lower (5%) / wrist center\n",
        "\n",
        "    x,y = [],[]\n",
        "    for i in range(slice5):\n",
        "        cnt = th_li[i + upper].count(255)\n",
        "        index = th_li[i + upper].index(255)\n",
        "        x.append([i+upper])\n",
        "        y.append([int((index*2 + cnt - 1)/2)])\n",
        "\n",
        "    # x, y / draw regression line\n",
        "\n",
        "    model = LinearRegression()\n",
        "    model.fit(X=x,y=y)\n",
        "\n",
        "    ####################################################\n",
        "\n",
        "    ## Rotation stage 02\n",
        "    \n",
        "    angle = math.atan2(h - 0, int(model.predict([[h]])) - int(model.predict([[0]])))*180/math.pi\n",
        "    M = cv2.getRotationMatrix2D((w/2,h/2), angle-90, 1)\n",
        "    rotate = cv2.warpAffine(before_rot_img, M, (w, h))\n",
        "\n",
        "    # Cutting img (rotated img)\n",
        "\n",
        "    for i in range(len(th[-1])):\n",
        "        if th[-1][i] == 255:\n",
        "            start_x = i\n",
        "            break\n",
        "\n",
        "    for i in range(len(th[-1])):\n",
        "        if th[-1][i] == 255:\n",
        "            end_x = i\n",
        "            \n",
        "\n",
        "    s_point = h - int((int(model.predict([[h]])-start_x)) * math.tan(math.pi*((90-angle)/180)))\n",
        "    e_point = h - int((end_x - int(model.predict([[h]]))) * math.tan(math.pi*((angle-90)/180)))\n",
        "    point = max(s_point, e_point)\n",
        "    rotated_img = rotate[:point]\n",
        "    return rotated_img\n",
        "\n",
        "######## Decompose_contrast ########\n",
        "def contrast_roi(img, low, high):\n",
        "\n",
        "    ## height, width\n",
        "\n",
        "    h, w = img.shape\n",
        "    img_ = np.zeros(img.shape, dtype=np.uint8)\n",
        "\n",
        "    for y in range(h):\n",
        "        for x in range(w):\n",
        "            temp = int((255 / (high - low)) * (img[y][x] - low))\n",
        "            if temp > 255:\n",
        "                img_[y][x] = 255\n",
        "            elif temp < 0:\n",
        "                img_[y][x] = 0\n",
        "            else:\n",
        "                img_[y][x] = temp\n",
        "    return img_\n",
        "\n",
        "######## Decompose_brightness ########\n",
        "def bright_ness(img):\n",
        "    ## columns, rows\n",
        "    cols, rows = img.shape[:2]\n",
        "    brightness = np.sum(img) / (255 * cols * rows)\n",
        "    return brightness\n",
        "\n",
        "######## Decomposing ########\n",
        "\n",
        "### img, morphology_value_1, morphology_value_2, filter_value(a,b)\n",
        "\n",
        "def Decomposing(rotated_img,a,b,d,e):\n",
        "\n",
        "    ######## Decomposing_stage_1 / [ Contours , Mask ] ########\n",
        "    decomp_img_1 = rotated_img.copy()\n",
        "\n",
        "    ## Adjusting brighness\n",
        "\n",
        "    if bright_ness(decomp_img_1) > 0.8:\n",
        "        decomp_img_1 = np.clip(decomp_img_1 - 80., 0, 255).astype(np.uint8)\n",
        "    elif bright_ness(decomp_img_1) > 0.75:\n",
        "        decomp_img_1 = np.clip(decomp_img_1 - 50., 0, 255).astype(np.uint8)\n",
        "    elif bright_ness(decomp_img_1) > 0.65:\n",
        "        decomp_img_1 = np.clip(decomp_img_1 - 30., 0, 255).astype(np.uint8)\n",
        "    else: decomp_img_1 = np.clip(decomp_img_1 - 10., 0, 255).astype(np.uint8)\n",
        "\n",
        "\n",
        "    ## change to Lab\n",
        "\n",
        "    decomp_img_1 = cv2.cvtColor(decomp_img_1, cv2.COLOR_RGB2BGR)\n",
        "    decomp_img_1 = cv2.cvtColor(decomp_img_1, cv2.COLOR_BGR2Lab)\n",
        "\n",
        "    ## Morphology\n",
        "\n",
        "    k = cv2.getStructuringElement(cv2.MORPH_CROSS, (a, a))\n",
        "    decomp_img_1 = cv2.morphologyEx(decomp_img_1, cv2.MORPH_TOPHAT, k) # Emphasis\n",
        "\n",
        "    ## Filter\n",
        "\n",
        "    decomp_img_1 = cv2.bilateralFilter(decomp_img_1,-1, d, e)\n",
        "\n",
        "    ## Lab to gray for binary\n",
        "\n",
        "    decomp_img_1 = cv2.cvtColor(decomp_img_1, cv2.COLOR_Lab2BGR)\n",
        "    decomp_img_1 = cv2.cvtColor(decomp_img_1, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    ## img_normalization\n",
        "\n",
        "    decomp_img_1 = cv2.normalize(decomp_img_1, None, 0, 255, cv2.NORM_MINMAX)\n",
        "\n",
        "    ## CLAHE\n",
        "\n",
        "    decomp_img_1 = cv2.equalizeHist(decomp_img_1)\n",
        "    clahe = cv2.createCLAHE(clipLimit=1.0, tileGridSize=(3,3)) \n",
        "    decomp_img_1= clahe.apply(decomp_img_1)          \n",
        "\n",
        "\n",
        "    ## Threshold / value = img.mean()\n",
        "    ret, mask = cv2.threshold(decomp_img_1,\n",
        "                            np.mean(decomp_img_1),\n",
        "                            255,\n",
        "                            cv2.THRESH_BINARY) \n",
        "\n",
        "    ## Extract object / same value pixels\n",
        "\n",
        "    contours, hierarchy = cv2.findContours(mask, \n",
        "                                            cv2.RETR_EXTERNAL, # only outline\n",
        "                                            cv2.CHAIN_APPROX_SIMPLE) # Contour vertex coordinate\n",
        "\n",
        "\n",
        "    ## drawing Contours\n",
        "\n",
        "    cv2.drawContours(mask, contours, -1, (255,255,255), -1) # -1: 모든 컨트어 표시 /color/ fill\n",
        "\n",
        "\n",
        "        \n",
        "    ######## Decomposing_stage_2 / [ Brightness_Empahsis ] ########\n",
        "\n",
        "\n",
        "\n",
        "    ## Empahsis\n",
        "    decomp_img_2 = rotated_img.copy()\n",
        "    if bright_ness(decomp_img_2) > 0.8:\n",
        "        decomp_img_2 = np.clip(decomp_img_2 - 80., 0, 255).astype(np.uint8)\n",
        "    elif bright_ness(decomp_img_2) > 0.75:\n",
        "        decomp_img_2 = np.clip(decomp_img_2 - 50., 0, 255).astype(np.uint8)\n",
        "    elif bright_ness(decomp_img_2) > 0.65:\n",
        "        decomp_img_2 = np.clip(decomp_img_2 - 30., 0, 255).astype(np.uint8)\n",
        "    else: decomp_img_2 = np.clip(decomp_img_2 - 10., 0, 255).astype(np.uint8)\n",
        "\n",
        "\n",
        "    ## Morphology\n",
        "    k2 = cv2.getStructuringElement(cv2.MORPH_CROSS,(b,b))\n",
        "    decomp_img_2 = cv2.morphologyEx(decomp_img_2, cv2.MORPH_TOPHAT, k2)\n",
        "\n",
        "    ## contrast\n",
        "    decomp_img_2 = cv2.cvtColor(decomp_img_2, cv2.COLOR_BGR2RGB)\n",
        "    decomp_img_2 = cv2.cvtColor(decomp_img_2, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    if decomp_img_2.mean() <= 15:\n",
        "        low = decomp_img_2.mean() * 3.2\n",
        "        high = decomp_img_2.mean() * 3.6\n",
        "    elif decomp_img_2.mean() <= 20:\n",
        "        low = decomp_img_2.mean() * 3\n",
        "        high = decomp_img_2.mean() * 3.6\n",
        "    else:\n",
        "        low = decomp_img_2.mean() * 3\n",
        "        high = decomp_img_2.mean() * 3.7\n",
        "\n",
        "    decomp_img_2 = cv2.blur(decomp_img_2,(2,2))\n",
        "    decomp_img_2 = contrast_roi(decomp_img_2, low, high)\n",
        "\n",
        "\n",
        "    ######## Decomposing_Final_stage / [ Result ] ########\n",
        "\n",
        "    ### Bone empahsis / bitwise (mask)\n",
        "\n",
        "    ## Morphology\n",
        "\n",
        "    ## Contours\n",
        "    contours, hierarchy = cv2.findContours(decomp_img_2, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    cv2.drawContours(decomp_img_2, contours, -1, (255, 255, 255), -1)\n",
        "\n",
        "    ## Bitwise (mask) / print white parts\n",
        "\n",
        "    decomp_img_2 = cv2.bitwise_and(decomp_img_2, mask) \n",
        "\n",
        "    decomp_img_2 = cv2.cvtColor(decomp_img_2, cv2.COLOR_GRAY2BGR)\n",
        "    decomp_img_2 = cv2.blur(decomp_img_2,(2,2))\n",
        "\n",
        "    bone_extraction = cv2.resize(decomp_img_2, (600, 800))\n",
        "\n",
        "    return bone_extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tR5TF_9nnV6"
      },
      "source": [
        "### Preprocessing Running Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daLecPEWnpxl"
      },
      "outputs": [],
      "source": [
        "def Bone_extraction(path):\n",
        "    try:\n",
        "        global bone_path\n",
        "        original_img = read_img(path)\n",
        "        mask = make_mask(original_img)\n",
        "        masked = cut_mask(original_img, mask)\n",
        "        overlaied_img = overlay_contours(masked)\n",
        "        rotated_img = img_rotation(overlaied_img)\n",
        "        bone = Decomposing(rotated_img,60,55,50,25)\n",
        "        bone_path = cv2.imwrite(path)\n",
        "        return bone_path\n",
        "\n",
        "    except:\n",
        "        print('ERROR > Please check again' )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWQsRh4unfwd"
      },
      "source": [
        "## Auto_processing Code ( No preprocessing Code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEUKMygIEKwp"
      },
      "outputs": [],
      "source": [
        "def input_info(NO, gender):\n",
        "  global img_path, model_path, tjnet_path\n",
        "  \n",
        "  # Gender condition\n",
        "  if gender == 1:\n",
        "    ge = 'M'\n",
        "    Gender_ = 'male'\n",
        "  elif gender == 0:\n",
        "    ge   = 'F'\n",
        "    Gender_ = 'female'\n",
        "\n",
        "  img_path =f'/content/drive/MyDrive/2차 프로젝트 원본 데이터/preprocessing_done/{Gender_}/{NO}_{ge}.jpg'\n",
        "  model_path = '/content/drive/MyDrive/2차 프로젝트 원본 데이터/yolov5'\n",
        "  tjnet_path = '/content/drive/MyDrive/2차 프로젝트 원본 데이터/TJM/tjnet24.h5'\n",
        "\n",
        "  return [NO, gender, img_path, model_path, tjnet_path]\n",
        "\n",
        "def predict_age(NO, gender):\n",
        "  import pandas as pd\n",
        "  df = pd.read_pickle('/content/drive/MyDrive/2차 프로젝트 원본 데이터/BA_all_.pkl')\n",
        "  # df.columns\n",
        "  ba_mean = df.BONE.mean()\n",
        "  ba_std = df.BONE.std()\n",
        "  return ba_mean, ba_std, df\n",
        "  \n",
        "\n",
        "def predict_zscore(gender, img_path, model_path,tjnet_path, show_crop, result_out):\n",
        "  yolo_crop_img(gender, img_path, model_path, show_crop=show_crop,result_out = result_out)\n",
        "  import numpy as np\n",
        "  import tensorflow.keras as tf\n",
        "  model = tf.models.load_model(tjnet_path, compile=False)\n",
        "  # grobal X : X_ray 이미지의 yolo crop image 값\n",
        "  y_predict = model.predict(X)\n",
        "  global pred\n",
        "  pred = y_predict[0][0]\n",
        "\n",
        "def yolo_crop_img(gender, img_path, model_path, result_out=False, show_crop=False):\n",
        "  import os\n",
        "  import numpy as np\n",
        "  os.chdir(model_path)\n",
        "  import torch\n",
        "  model = torch.load('./model.pt', map_location='cpu')\n",
        "  result = model(img_path)\n",
        "  global crops\n",
        "  crops = result.crop(save=False)\n",
        "  global X\n",
        "  X = out_crop_img(crops, gender)\n",
        "  global img\n",
        "  img = np.squeeze(result.render())\n",
        "  if show_crop: show_img(result)\n",
        "  if result_out: return result\n",
        "\n",
        "def show_img(result):\n",
        "  import matplotlib.pyplot as plt\n",
        "  import numpy as np\n",
        "  %matplotlib inline\n",
        "  plt.figure(figsize=(16,12))\n",
        "  plt.imshow(np.squeeze(result.render()))\n",
        "  plt.show()\n",
        "\n",
        "def out_crop_img(crop, gender):\n",
        "  import re\n",
        "  import cv2\n",
        "  import numpy as np\n",
        "\n",
        "  gender = np.array(gender).reshape(1,1)\n",
        "\n",
        "  for i in range(7):\n",
        "    carpal = re.compile('CARPAL.')\n",
        "    ip = re.compile('IP.')\n",
        "    lmcp = re.compile('LMCP.')\n",
        "    lpip = re.compile('LPIP.')\n",
        "    mmcp = re.compile('MMCP.')\n",
        "    mpip = re.compile('MPIP.')\n",
        "    tmcp = re.compile('TMCP.')\n",
        "\n",
        "    if carpal.search(crop[i]['label']):\n",
        "      CARPAL_img = crop[i]['im']\n",
        "      CARPAL_img = cv2.resize(CARPAL_img, (224,224),cv2.INTER_AREA)\n",
        "      CARPAL_img = np.expand_dims(CARPAL_img, axis=0)\n",
        "\n",
        "    if ip.search(crop[i]['label']):\n",
        "      IP_img = crop[i]['im']\n",
        "      IP_img = cv2.resize(IP_img, (75,75),cv2.INTER_AREA)\n",
        "      IP_img = np.expand_dims(IP_img, axis=0)\n",
        "      \n",
        "    if lmcp.search(crop[i]['label']):\n",
        "      LMCP_img = crop[i]['im']\n",
        "      LMCP_img = cv2.resize(LMCP_img, (75,75),cv2.INTER_AREA)\n",
        "      LMCP_img = np.expand_dims(LMCP_img, axis=0)\n",
        "\n",
        "    if lpip.search(crop[i]['label']):\n",
        "      LPIP_img = crop[i]['im']\n",
        "      LPIP_img = cv2.resize(LPIP_img, (75,75),cv2.INTER_AREA)\n",
        "      LPIP_img = np.expand_dims(LPIP_img, axis=0)\n",
        "    \n",
        "    if mmcp.search(crop[i]['label']):\n",
        "      MMCP_img = crop[i]['im']\n",
        "      MMCP_img = cv2.resize(MMCP_img, (75,75),cv2.INTER_AREA)\n",
        "      MMCP_img = np.expand_dims(MMCP_img, axis=0)\n",
        "    \n",
        "    if mpip.search(crop[i]['label']):\n",
        "      MPIP_img = crop[i]['im']\n",
        "      MPIP_img = cv2.resize(MPIP_img, (75,75),cv2.INTER_AREA)\n",
        "      MPIP_img = np.expand_dims(MPIP_img, axis=0)\n",
        "\n",
        "    if tmcp.search(crop[i]['label']):\n",
        "      TMCP_img = crop[i]['im']\n",
        "      TMCP_img = cv2.resize(TMCP_img, (75,75),cv2.INTER_AREA)\n",
        "      TMCP_img = np.expand_dims(TMCP_img, axis=0)\n",
        "\n",
        "    else : continue\n",
        "\n",
        "  \n",
        "  return [CARPAL_img, LMCP_img, MMCP_img,TMCP_img, LPIP_img, MPIP_img, IP_img, gender]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPCZ6XIknrLa"
      },
      "source": [
        "### Auto Processing Running Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TEpSRtdBD44"
      },
      "outputs": [],
      "source": [
        "def final(NO,gender,show_crop=True,result_out=True):\n",
        "    NO, gender, img_path, model_path, tjnet_path = input_info(NO,gender)\n",
        "    bone, save_path = Bone_extraction(img_path)\n",
        "    ba_mean, ba_std, df = predict_age(NO,gender)\n",
        "    predict_zscore(gender, save_path, model_path,tjnet_path, show_crop, result_out)\n",
        "    prediction_BA = (pred * ba_std + ba_mean)/12\n",
        "    cond1 = df['GENDER'] == gender\n",
        "    cond2 = df['NO'] == NO\n",
        "    Actual_value = df[cond1&cond2]['BA_TOTAL'].values[0]\n",
        "    Diff = abs( Actual_value - prediction_BA )*12\n",
        "    print('Actual > {0} Year'.format( round(Actual_value,2)) )\n",
        "    print('prediction_BA > {0} Year'.format( round(prediction_BA,2)))\n",
        "    print('Different > {0} Month'.format(round(Diff,2)))\n",
        "    return Diff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXFBmVWkrUCr"
      },
      "source": [
        "# Validation Acurracy Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j10KdaVYjxNf"
      },
      "outputs": [],
      "source": [
        "def Total_pred_validation(NO,gender,show_crop=True,result_out=True):\n",
        "\n",
        "########################### * Validation Accuracy Function *  ################################\n",
        "##                                                                                          ##\n",
        "##                                  * Functions *                                           ##\n",
        "##        1. input_info : Information ( NO : index number, Gender : patient's gender )      ##\n",
        "##        2. Bone_extraction : Extract bone from original x_ray img                         ##\n",
        "##          2-1. read_img : Read img from path                                              ##\n",
        "##          2-2. make_mask : Making mask for removing background                            ##\n",
        "##          2-3. cut_mask : Cutting background based mask                                   ##\n",
        "##          2-4. overlay_contours : Overlaying Contours                                     ##\n",
        "##          2-5. img_rotation : Rotation                                                    ##\n",
        "##          2-6. contrast_roi : Contrast for Decomposing                                    ##\n",
        "##          2-7. bright_ness : Changing Brightness for Decomposing                          ##\n",
        "##          2-8. Decomposing : Extracting Bone                                              ##\n",
        "##        3. predict_age : Calculation Bone Age mean, std values in train_DataFrame         ##\n",
        "##        4. predict_zscore : Using tjnet, predict Bone Age z_score value                   ##\n",
        "##        5. yolo_crop_img : Cropping Img Using YoloV5                                      ##\n",
        "##        6. show_img : Print Result Img                                                    ##\n",
        "##        7. out_crop_img : Declaration Variable of crop images                             ##                     \n",
        "##                                                                                          ##\n",
        "##                                    * Notes *                                             ##\n",
        "##           Several Time will be spent on loading Yolo model and TJnet model.              ##\n",
        "##                                                                                          ##\n",
        "##############################################################################################\n",
        "    \n",
        "    ## 1. input_info\n",
        "\n",
        "    def input_info(NO, gender):\n",
        "        global img_path, model_path, tjnet_path\n",
        "\n",
        "        # Gender condition\n",
        "        if gender == 1:\n",
        "            ge = 'M'\n",
        "            Gender_ = 'Male'\n",
        "        elif gender == 0:\n",
        "            ge   = 'F'\n",
        "            Gender_ = 'Female'\n",
        "\n",
        "        img_path =f'/content/drive/MyDrive/2차 프로젝트 원본 데이터/데이터 원본/image/{Gender_}/{NO}_{ge}.jpg'\n",
        "        model_path = '/content/drive/MyDrive/2차 프로젝트 원본 데이터/yolov5'\n",
        "        tjnet_path = '/content/drive/MyDrive/2차 프로젝트 원본 데이터/TJM/tjnet24.h5'\n",
        "\n",
        "        return [NO, gender, img_path, model_path, tjnet_path]\n",
        "\n",
        "    ## 2. Bone_extraction\n",
        "\n",
        "    def Bone_extraction(img_path):\n",
        "\n",
        "        # model import\n",
        "        import numpy as np\n",
        "        import cv2\n",
        "        import matplotlib.pyplot as plt\n",
        "        from sklearn.linear_model import LinearRegression\n",
        "        import glob\n",
        "        import math\n",
        "\n",
        "        ## 2-1. read_img\n",
        "        def read_img(path):\n",
        "            original_img = cv2.imread(path)\n",
        "            return original_img\n",
        "\n",
        "        ## 2-2. make_mask\n",
        "        def make_mask(original_img):\n",
        "            # change to lab for making mask\n",
        "            img_mask = original_img.copy()\n",
        "            img_mask = cv2.cvtColor(img_mask, cv2.COLOR_RGB2BGR)\n",
        "            img_mask = cv2.cvtColor(img_mask, cv2.COLOR_BGR2Lab)\n",
        "            ## blur _02 \n",
        "            # kernel_size = odds / value = img.mean()\n",
        "            blur_k = int((img_mask.mean()*0.5)//2)*2+1\n",
        "            img_mask = cv2.medianBlur(img_mask, blur_k)\n",
        "            ## change to Grayscale for threshold\n",
        "            img_mask = cv2.cvtColor(img_mask, cv2.COLOR_Lab2BGR)\n",
        "            img_mask = cv2.cvtColor(img_mask, cv2.COLOR_BGR2GRAY)\n",
        "            ## binary / value = img.mean()\n",
        "            if img_mask.mean() > 100 : \n",
        "                th = img_mask.mean()*0.94\n",
        "            else : \n",
        "                th = img_mask.mean()\n",
        "            ret, img_mask = cv2.threshold(img_mask, th, 255, cv2.THRESH_BINARY)\n",
        "            ## mask based Max value of contours\n",
        "            contours, hierarchy = cv2.findContours(img_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "            max_cnt = max(contours, key=cv2.contourArea)\n",
        "            mask = np.zeros(img_mask.shape, dtype=np.uint8)\n",
        "            cv2.drawContours(mask, [max_cnt], -1, (255,255,255), -1)\n",
        "            ## Applying for dilation\n",
        "            k = cv2.getStructuringElement(cv2.MORPH_RECT, (8,8))\n",
        "            mask = cv2.dilate(mask,k)\n",
        "            return mask\n",
        "\n",
        "        ## 2-3. cut_mask\n",
        "        def cut_mask(original_img, mask):\n",
        "            ## copying\n",
        "            img_for_cut = original_img.copy()\n",
        "            ## H/W\n",
        "            height, width = img_for_cut.shape[:2]\n",
        "            ## mask\n",
        "            mask_list = mask.tolist()    \n",
        "            for y in range(int(height*0.05),height):\n",
        "                if max(mask[y,int(width*0.3):int(width*0.7)]) > 0:\n",
        "                    start_y = y-int(height*0.05)\n",
        "                    break            \n",
        "            for x in range(int(width*0.05),width):\n",
        "                if max(mask[int(height*0.3):int(height*0.7),x]) > 0:\n",
        "                    start_x = x-int(width*0.05)\n",
        "                    break           \n",
        "            for x in range(int(width*0.95),-1,-1):\n",
        "                if max(mask[int(height*0.3):int(height*0.7),x]) > 0:\n",
        "                    end_x = x+int(width*0.05)\n",
        "                    break               \n",
        "            cut_index = 0\n",
        "            if mask_list[height-1][-1] == 255 or mask_list[height-1][0] == 255:\n",
        "                for n in reversed(range(height)):\n",
        "                    if mask_list[n][0] == 0 or mask_list[n][-1] == 0:\n",
        "                        cut_index = n\n",
        "                        break            \n",
        "            if cut_index == 0:\n",
        "                cut_index = height\n",
        "            ## converting color\n",
        "            img_for_cut = cv2.cvtColor(img_for_cut, cv2.COLOR_BGR2GRAY) \n",
        "            img_for_cut = img_for_cut[start_y:(cut_index-1),start_x:end_x]\n",
        "            mask = mask[start_y:(cut_index-1),start_x:end_x]\n",
        "            ## remove background\n",
        "            masked = cv2.bitwise_and(img_for_cut, mask)\n",
        "            return masked\n",
        "        \n",
        "        ## 2-4. overlay_contours\n",
        "        def overlay_contours(masked):\n",
        "            copied_img = masked.copy()\n",
        "            ## smoothing\n",
        "            gray_smooth = cv2.bilateralFilter(copied_img, 7,60,60)\n",
        "            ## contours\n",
        "            outline = cv2.Canny(gray_smooth,50,50)\n",
        "            ## overlay\n",
        "            overlaied_img = cv2.add(masked,outline)\n",
        "            return overlaied_img\n",
        "\n",
        "        ## 2-5. img_rotation\n",
        "        def img_rotation(overlaied_img):\n",
        "            ## copying img\n",
        "            before_rot_img = overlaied_img.copy()\n",
        "            h, w = before_rot_img.shape[:2]\n",
        "            before_rot_img = cv2.cvtColor(before_rot_img, cv2.COLOR_RGB2BGR)\n",
        "            gray = cv2.cvtColor(before_rot_img, cv2.COLOR_BGR2GRAY)\n",
        "            ret, th = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
        "            th_li = th.tolist()\n",
        "            ## Rotation stage 01\n",
        "            # lower = first black spot\n",
        "            for i in reversed(range(h)):\n",
        "                if th_li[i][0] == 0 and th_li[i][-1] == 0:\n",
        "                    lower = i\n",
        "                    break\n",
        "            # lower = condition ; bottom = lower / img * 0.95\n",
        "            if lower == h - 1:\n",
        "                lower = int(h*0.9)\n",
        "            # upper = condition ; lower + lower * 0.05\n",
        "            slice5 = int(len(th)*0.05)\n",
        "            upper = lower - slice5\n",
        "            # x, y = between upper and lower (5%) / wrist center\n",
        "            x,y = [],[]\n",
        "            for i in range(slice5):\n",
        "                cnt = th_li[i + upper].count(255)\n",
        "                index = th_li[i + upper].index(255)\n",
        "                x.append([i+upper])\n",
        "                y.append([int((index*2 + cnt - 1)/2)])\n",
        "            # x, y / draw regression line\n",
        "            model = LinearRegression()\n",
        "            model.fit(X=x,y=y)\n",
        "            ####################################################\n",
        "            ## Rotation stage 02        \n",
        "            angle = math.atan2(h - 0, int(model.predict([[h]])) - int(model.predict([[0]])))*180/math.pi\n",
        "            M = cv2.getRotationMatrix2D((w/2,h/2), angle-90, 1)\n",
        "            rotate = cv2.warpAffine(before_rot_img, M, (w, h))\n",
        "            # Cutting img (rotated img)\n",
        "            for i in range(len(th[-1])):\n",
        "                if th[-1][i] == 255:\n",
        "                    start_x = i\n",
        "                    break\n",
        "            for i in range(len(th[-1])):\n",
        "                if th[-1][i] == 255:\n",
        "                    end_x = i                   \n",
        "            s_point = h - int((int(model.predict([[h]])-start_x)) * math.tan(math.pi*((90-angle)/180)))\n",
        "            e_point = h - int((end_x - int(model.predict([[h]]))) * math.tan(math.pi*((angle-90)/180)))\n",
        "            point = max(s_point, e_point)\n",
        "            rotated_img = rotate[:point]\n",
        "            return rotated_img\n",
        "\n",
        "        ## 2-6. contrast_roi\n",
        "        def contrast_roi(img, low, high):\n",
        "            ## height, width\n",
        "            h, w = img.shape\n",
        "            img_ = np.zeros(img.shape, dtype=np.uint8)\n",
        "            for y in range(h):\n",
        "                for x in range(w):\n",
        "                    temp = int((255 / (high - low)) * (img[y][x] - low))\n",
        "                    if temp > 255:\n",
        "                        img_[y][x] = 255\n",
        "                    elif temp < 0:\n",
        "                        img_[y][x] = 0\n",
        "                    else:\n",
        "                        img_[y][x] = temp\n",
        "            return img_\n",
        "\n",
        "        ## 2-7. bright_ness\n",
        "        def bright_ness(img):\n",
        "            ## columns, rows\n",
        "            cols, rows = img.shape[:2]\n",
        "            brightness = np.sum(img) / (255 * cols * rows)\n",
        "            return brightness\n",
        "\n",
        "        ## 2-8. Decomposing\n",
        "        def Decomposing(rotated_img,a,b,d,e):\n",
        "            ######## Decomposing_stage_1 / [ Contours , Mask ] ########\n",
        "            decomp_img_1 = rotated_img.copy()\n",
        "            ## Adjusting brighness\n",
        "            if bright_ness(decomp_img_1) > 0.8:\n",
        "                decomp_img_1 = np.clip(decomp_img_1 - 80., 0, 255).astype(np.uint8)\n",
        "            elif bright_ness(decomp_img_1) > 0.75:\n",
        "                decomp_img_1 = np.clip(decomp_img_1 - 50., 0, 255).astype(np.uint8)\n",
        "            elif bright_ness(decomp_img_1) > 0.65:\n",
        "                decomp_img_1 = np.clip(decomp_img_1 - 30., 0, 255).astype(np.uint8)\n",
        "            else: decomp_img_1 = np.clip(decomp_img_1 - 10., 0, 255).astype(np.uint8)\n",
        "            ## change to Lab\n",
        "            decomp_img_1 = cv2.cvtColor(decomp_img_1, cv2.COLOR_RGB2BGR)\n",
        "            decomp_img_1 = cv2.cvtColor(decomp_img_1, cv2.COLOR_BGR2Lab)\n",
        "            ## Morphology\n",
        "            k = cv2.getStructuringElement(cv2.MORPH_CROSS, (a, a))\n",
        "            decomp_img_1 = cv2.morphologyEx(decomp_img_1, cv2.MORPH_TOPHAT, k) # Emphasis\n",
        "            ## Filter\n",
        "            decomp_img_1 = cv2.bilateralFilter(decomp_img_1,-1, d, e)\n",
        "            ## Lab to gray for binary\n",
        "            decomp_img_1 = cv2.cvtColor(decomp_img_1, cv2.COLOR_Lab2BGR)\n",
        "            decomp_img_1 = cv2.cvtColor(decomp_img_1, cv2.COLOR_BGR2GRAY)\n",
        "            ## img_normalization\n",
        "            decomp_img_1 = cv2.normalize(decomp_img_1, None, 0, 255, cv2.NORM_MINMAX)\n",
        "            ## CLAHE\n",
        "            decomp_img_1 = cv2.equalizeHist(decomp_img_1)\n",
        "            clahe = cv2.createCLAHE(clipLimit=1.0, tileGridSize=(3,3)) \n",
        "            decomp_img_1= clahe.apply(decomp_img_1)          \n",
        "            ## Threshold / value = img.mean()\n",
        "            ret, mask = cv2.threshold(decomp_img_1,\n",
        "                                    np.mean(decomp_img_1),\n",
        "                                    255,\n",
        "                                    cv2.THRESH_BINARY) \n",
        "            ## Extract object / same value pixels\n",
        "            contours, hierarchy = cv2.findContours(mask, \n",
        "                                                    cv2.RETR_EXTERNAL, # only outline\n",
        "                                                    cv2.CHAIN_APPROX_SIMPLE) # Contour vertex coordinate\n",
        "            ## drawing Contours\n",
        "            cv2.drawContours(mask, contours, -1, (255,255,255), -1) # -1: 모든 컨트어 표시 /color/ fill             \n",
        "            ######## Decomposing_stage_2 / [ Brightness_Empahsis ] ########\n",
        "            ## Empahsis\n",
        "            decomp_img_2 = rotated_img.copy()\n",
        "            if bright_ness(decomp_img_2) > 0.8:\n",
        "                decomp_img_2 = np.clip(decomp_img_2 - 80., 0, 255).astype(np.uint8)\n",
        "            elif bright_ness(decomp_img_2) > 0.75:\n",
        "                decomp_img_2 = np.clip(decomp_img_2 - 50., 0, 255).astype(np.uint8)\n",
        "            elif bright_ness(decomp_img_2) > 0.65:\n",
        "                decomp_img_2 = np.clip(decomp_img_2 - 30., 0, 255).astype(np.uint8)\n",
        "            else: decomp_img_2 = np.clip(decomp_img_2 - 10., 0, 255).astype(np.uint8)\n",
        "            ## Morphology\n",
        "            k2 = cv2.getStructuringElement(cv2.MORPH_CROSS,(b,b))\n",
        "            decomp_img_2 = cv2.morphologyEx(decomp_img_2, cv2.MORPH_TOPHAT, k2)\n",
        "            ## contrast\n",
        "            decomp_img_2 = cv2.cvtColor(decomp_img_2, cv2.COLOR_BGR2RGB)\n",
        "            decomp_img_2 = cv2.cvtColor(decomp_img_2, cv2.COLOR_BGR2GRAY)\n",
        "            if decomp_img_2.mean() <= 15:\n",
        "                low = decomp_img_2.mean() * 3.2\n",
        "                high = decomp_img_2.mean() * 3.6\n",
        "            elif decomp_img_2.mean() <= 20:\n",
        "                low = decomp_img_2.mean() * 3\n",
        "                high = decomp_img_2.mean() * 3.6\n",
        "            else:\n",
        "                low = decomp_img_2.mean() * 3\n",
        "                high = decomp_img_2.mean() * 3.7\n",
        "            decomp_img_2 = cv2.blur(decomp_img_2,(2,2))\n",
        "            decomp_img_2 = contrast_roi(decomp_img_2, low, high)\n",
        "            ######## Decomposing_Final_stage / [ Result ] ########\n",
        "            ### Bone empahsis / bitwise (mask)\n",
        "            ## Morphology\n",
        "            ## Contours\n",
        "            contours, hierarchy = cv2.findContours(decomp_img_2, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "            cv2.drawContours(decomp_img_2, contours, -1, (255, 255, 255), -1)\n",
        "            ## Bitwise (mask) / print white parts\n",
        "            decomp_img_2 = cv2.bitwise_and(decomp_img_2, mask) \n",
        "            decomp_img_2 = cv2.cvtColor(decomp_img_2, cv2.COLOR_GRAY2BGR)\n",
        "            decomp_img_2 = cv2.blur(decomp_img_2,(2,2))\n",
        "            bone_extraction = cv2.resize(decomp_img_2, (600, 800))\n",
        "            return bone_extraction\n",
        "\n",
        "        ## Bone_extraction (Running code)\n",
        "        try:\n",
        "            global bone\n",
        "            original_img = read_img(img_path)\n",
        "            mask = make_mask(original_img)\n",
        "            masked = cut_mask(original_img, mask)\n",
        "            # overlaied_img = overlay_contours(masked)\n",
        "            rotated_img = img_rotation(masked)\n",
        "            bone = Decomposing(rotated_img,60,55,50,25)\n",
        "            \n",
        "            # Save extraction img\n",
        "            global save_path\n",
        "            import re\n",
        "            regex = re.compile('[^.+]+')\n",
        "            test = regex.findall(img_path)\n",
        "            save_path = test[0] + '_extraction' + '.jpg'\n",
        "            cv2.imwrite(save_path, bone)\n",
        "            return bone, save_path\n",
        "\n",
        "        except:\n",
        "            print('ERROR > Please check again' )\n",
        "\n",
        "\n",
        "    ## 3. predict_age\n",
        "\n",
        "    def predict_age(NO, gender):\n",
        "        import pandas as pd\n",
        "        df = pd.read_pickle('/content/drive/MyDrive/2차 프로젝트 원본 데이터/BA_all_.pkl')\n",
        "        ba_mean = df.BONE.mean()\n",
        "        ba_std = df.BONE.std()\n",
        "        return ba_mean, ba_std, df\n",
        "\n",
        "    ## 4. predict_zscore\n",
        "\n",
        "    def predict_zscore(gender, save_path, model_path,tjnet_path, show_crop, result_out):\n",
        "\n",
        "        # Function connection line ; yolo_crop_img(4)\n",
        "        yolo_crop_img(gender, save_path, model_path, show_crop=show_crop,result_out = result_out)\n",
        "\n",
        "        import numpy as np\n",
        "        import tensorflow.keras as tf\n",
        "\n",
        "        model = tf.models.load_model(tjnet_path, compile=False)\n",
        "\n",
        "        # grobal X : X_ray 이미지의 yolo crop image 값\n",
        "\n",
        "        y_predict = model.predict(X)\n",
        "        global pred\n",
        "        pred = y_predict[0][0] \n",
        "\n",
        "    ## 5. yolo_crop_img\n",
        "\n",
        "    def yolo_crop_img(gender, save_path, model_path, result_out=False, show_crop=False):\n",
        "        import os\n",
        "        import numpy as np\n",
        "\n",
        "        os.chdir(model_path)\n",
        "        import torch\n",
        "    \n",
        "        model = torch.load('./model.pt', map_location='cpu')\n",
        "        result = model(save_path)\n",
        "        global crops\n",
        "        crops = result.crop(save=False)\n",
        "        global X\n",
        "        X = out_crop_img(crops, gender)\n",
        "        global img\n",
        "        img = np.squeeze(result.render())\n",
        "        if show_crop: show_img(result)\n",
        "        if result_out: return result\n",
        "\n",
        "    ## 6. show_img\n",
        "\n",
        "    def show_img(result):\n",
        "        import matplotlib.pyplot as plt\n",
        "        import numpy as np\n",
        "        %matplotlib inline\n",
        "        plt.figure(figsize=(16,12))\n",
        "        plt.imshow(np.squeeze(result.render()))\n",
        "        plt.show()\n",
        "\n",
        "    ## 7. out_crop_img\n",
        "\n",
        "    def out_crop_img(crop, gender):\n",
        "        import re\n",
        "        import cv2\n",
        "        import numpy as np\n",
        "\n",
        "        gender = np.array(gender).reshape(1,1)\n",
        "\n",
        "        for i in range(7):\n",
        "            carpal = re.compile('CARPAL.')\n",
        "            ip = re.compile('IP.')\n",
        "            lmcp = re.compile('LMCP.')\n",
        "            lpip = re.compile('LPIP.')\n",
        "            mmcp = re.compile('MMCP.')\n",
        "            mpip = re.compile('MPIP.')\n",
        "            tmcp = re.compile('TMCP.')\n",
        "\n",
        "            if carpal.search(crop[i]['label']):\n",
        "                CARPAL_img = crop[i]['im']\n",
        "                CARPAL_img = cv2.resize(CARPAL_img, (224,224),cv2.INTER_AREA)\n",
        "                CARPAL_img = np.expand_dims(CARPAL_img, axis=0)\n",
        "\n",
        "            if ip.search(crop[i]['label']):\n",
        "                IP_img = crop[i]['im']\n",
        "                IP_img = cv2.resize(IP_img, (75,75),cv2.INTER_AREA)\n",
        "                IP_img = np.expand_dims(IP_img, axis=0)\n",
        "                \n",
        "            if lmcp.search(crop[i]['label']):\n",
        "                LMCP_img = crop[i]['im']\n",
        "                LMCP_img = cv2.resize(LMCP_img, (75,75),cv2.INTER_AREA)\n",
        "                LMCP_img = np.expand_dims(LMCP_img, axis=0)\n",
        "\n",
        "            if lpip.search(crop[i]['label']):\n",
        "                LPIP_img = crop[i]['im']\n",
        "                LPIP_img = cv2.resize(LPIP_img, (75,75),cv2.INTER_AREA)\n",
        "                LPIP_img = np.expand_dims(LPIP_img, axis=0)\n",
        "            \n",
        "            if mmcp.search(crop[i]['label']):\n",
        "                MMCP_img = crop[i]['im']\n",
        "                MMCP_img = cv2.resize(MMCP_img, (75,75),cv2.INTER_AREA)\n",
        "                MMCP_img = np.expand_dims(MMCP_img, axis=0)\n",
        "                \n",
        "            if mpip.search(crop[i]['label']):\n",
        "                MPIP_img = crop[i]['im']\n",
        "                MPIP_img = cv2.resize(MPIP_img, (75,75),cv2.INTER_AREA)\n",
        "                MPIP_img = np.expand_dims(MPIP_img, axis=0)\n",
        "\n",
        "            if tmcp.search(crop[i]['label']):\n",
        "                TMCP_img = crop[i]['im']\n",
        "                TMCP_img = cv2.resize(TMCP_img, (75,75),cv2.INTER_AREA)\n",
        "                TMCP_img = np.expand_dims(TMCP_img, axis=0)\n",
        "\n",
        "            else : continue\n",
        "\n",
        "        return [CARPAL_img, LMCP_img, MMCP_img,TMCP_img, LPIP_img, MPIP_img, IP_img, gender]\n",
        "\n",
        "#### All Functions line ####\n",
        "\n",
        "    NO, gender, img_path, model_path, tjnet_path = input_info(NO,gender)\n",
        "    bone, save_path = Bone_extraction(img_path)\n",
        "    ba_mean, ba_std, df = predict_age(NO,gender)\n",
        "    predict_zscore(gender, save_path, model_path,tjnet_path, show_crop, result_out)\n",
        "    prediction_BA = (pred * ba_std + ba_mean)/12\n",
        "    cond1 = df['GENDER'] == gender\n",
        "    cond2 = df['NO'] == NO\n",
        "    Actual_value = df[cond1&cond2]['BA_TOTAL'].values[0]\n",
        "    Diff = abs( Actual_value - prediction_BA )*12\n",
        "    print('Actual > {0} Year'.format( round(Actual_value,2)) )\n",
        "    print('prediction_BA > {0} Year'.format( round(prediction_BA,2)))\n",
        "    print('Different > {0} Month'.format(round(Diff,2)))\n",
        "    return Diff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOW2aFean46e"
      },
      "source": [
        "### Validation Accuracy Code _ Test code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMsuzVjfn-wj"
      },
      "outputs": [],
      "source": [
        "Total_pred_validation(1,0,True,True)\n",
        "# orginal_img, no=1, gender = 0(female) , showing img = True, Return img data = True\n",
        "# no apply canny_edge\n",
        "# img_path =f'/content/drive/MyDrive/2차 프로젝트 원본 데이터/preprocessing_done/{Gender_}/{NO}_{ge}.jpg'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhJXE8E49cFX"
      },
      "source": [
        "### Blend_Altman"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YemZjLPXOq7M"
      },
      "source": [
        "##### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BQF4wnx9e9g"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "## For Loop\n",
        "import pandas as pd\n",
        "import re\n",
        "regex = re.compile('[^.+]+')\n",
        "regex2 = re.compile(r'\\d+')\n",
        "\n",
        "import os\n",
        "path = '/content/drive/MyDrive/2차 프로젝트 원본 데이터/B_A_img_data/'\n",
        "dir_list = os.listdir(path)\n",
        "# print(dir_list)\n",
        "path_list = []\n",
        "for p in dir_list:\n",
        "    path_list.append(path+p)\n",
        "\n",
        "\n",
        "prediction = []\n",
        "nope = []\n",
        "from tqdm import tqdm\n",
        "for path in tqdm(path_list):\n",
        "    try:\n",
        "        if '_F' in path:\n",
        "            gender = 0\n",
        "        elif '_M' in path:\n",
        "            gender = 1\n",
        "\n",
        "        img_path = path \n",
        "\n",
        "        df = pd.read_pickle('/content/drive/MyDrive/2차 프로젝트 원본 데이터/BA_all_.pkl')    \n",
        "        test = regex.findall(path)\n",
        "        text = regex2.findall(test[0]) \n",
        "        # A = df[ (df['NO']==int(text[1])) & (df['GENDER']==gender) ][['NO','BA_1','BA_2','BA_TOTAL']]\n",
        "        print()\n",
        "        print('INFO > ', path)\n",
        "        print('NO > ', text[1], 'Gender > ', gender)\n",
        "        # display('Real_age_in_df > ', A)\n",
        "        Total_pred_gui(gender)\n",
        "        # print('prediction_BA > {0} Year'.format( round(prediction_BA,2)))\n",
        "\n",
        "        pred = round(prediction_BA,2) \n",
        "        Result = str(text[1])+','+str(gender)+','+str(pred)\n",
        "        prediction.append(Result)\n",
        "    except:\n",
        "        nope.append(path)\n",
        "        print('안됨')\n",
        "        print(path)\n",
        "\n",
        "display(prediction)\n",
        "nope"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqwC0RzkOzSX"
      },
      "source": [
        "##### DataFrame for Comparing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnXc30tyT_8G"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "validation_result = pd.read_csv('/content/drive/MyDrive/2차 프로젝트 원본 데이터/validation_result.csv',index_col=0)\n",
        "validation_result = validation_result.rename(columns={'0': 'value'})\n",
        "validation_result = validation_result['value'].str.split(',', expand = True)\n",
        "validation_result = validation_result.rename(columns = {0 : 'NO', 1: 'GENDER', 2: 'PREDICTION'})\n",
        "\n",
        "val_df = pd.read_pickle('/content/drive/MyDrive/2차 프로젝트 원본 데이터/val.pkl')\n",
        "val_df = val_df[['NO','GENDER','DIG_AGE','BA_1','BA_2','BA_TOTAL']]\n",
        "\n",
        "validation_result['NO'] = validation_result['NO'].astype(int)\n",
        "validation_result['GENDER'] = validation_result['GENDER'].astype(int)\n",
        "validation_result['PREDICTION'] = validation_result['PREDICTION'].astype(float)\n",
        "Compare_df = pd.merge(val_df, validation_result, on=['NO','GENDER'],how='left')\n",
        "Compare_df.info()\n",
        "\n",
        "# ERROR 제외\n",
        "Compare_df[Compare_df['NO']==490] # 15\n",
        "Compare_df[Compare_df['NO']==196] # 41\n",
        "Compare_df[Compare_df['NO']==217] # 45\n",
        "Compare_df=Compare_df.drop([15,41,45])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Xcsj6trTz-d"
      },
      "outputs": [],
      "source": [
        "Compare_df['1-PRED'] = Compare_df['BA_1'] - Compare_df['PREDICTION']\n",
        "Compare_df['2-PRED'] = Compare_df['BA_2'] - Compare_df['PREDICTION']\n",
        "Compare_df['TOTAL-PRED'] = Compare_df['BA_TOTAL'] - Compare_df['PREDICTION']\n",
        "\n",
        "# # 저장\n",
        "# Compare_df.to_csv('/content/drive/MyDrive/2차 프로젝트 원본 데이터/Comparing_df.csv')\n",
        "Compare_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AR2j1DtKTgRM"
      },
      "source": [
        "#### BLEND_ALTMAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnNuDK6TMlYU"
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "f, ax = plt.subplots(1, figsize = (8,5))\n",
        "sm.graphics.mean_diff_plot(Compare_df['PREDICTION'],\n",
        "                           Compare_df['BA_1'],\n",
        "                           ax = ax)\n",
        "plt.title('BA_1 vs PREDICTION')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2kQbptIToSa"
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "f, ax = plt.subplots(1, figsize = (8,5))\n",
        "sm.graphics.mean_diff_plot(Compare_df['PREDICTION'],\n",
        "                           Compare_df['BA_2'],\n",
        "                           ax = ax)\n",
        "plt.title('BA_2 vs PREDICTION')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StbpPgswTtQm"
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "f, ax = plt.subplots(1, figsize = (8,5))\n",
        "sm.graphics.mean_diff_plot(Compare_df['PREDICTION'],\n",
        "                           Compare_df['BA_TOTAL'],\n",
        "                           ax = ax)\n",
        "plt.title('BA_TOTAL vs PREDICTION')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCOtgF5U8kgO"
      },
      "source": [
        "# GUI code\n",
        "- show_crop / result_out no function\n",
        "- show_img function can operation separately"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTLN8oSisVcA"
      },
      "outputs": [],
      "source": [
        "# img_path = uploading img path\n",
        "# gender = inputing info\n",
        "def Total_pred_gui(gender):\n",
        "\n",
        "###########################  * Prediction Function for GUI *  ################################\n",
        "##                                                                                          ##\n",
        "##                                  * Functions *                                           ##\n",
        "##        1. input_info Information ( NO : index number, Gender : patient's gender )        ##\n",
        "##        2. Bone_extraction : Extract bone from original x_ray img                         ##\n",
        "##          2-1. read_img : Read img from path                                              ##\n",
        "##          2-2. make_mask : Making mask for removing background                            ##\n",
        "##          2-3. cut_mask : Cutting background based mask                                   ##\n",
        "##          2-4. overlay_contours : Overlaying Contours                                     ##\n",
        "##          2-5. img_rotation : Rotation                                                    ##\n",
        "##          2-6. contrast_roi : Contrast for Decomposing                                    ##\n",
        "##          2-7. bright_ness : Changing Brightness for Decomposing                          ##\n",
        "##          2-8. Decomposing : Extracting Bone                                              ##\n",
        "##        3. predict_zscore : Using tjnet, predict Bone Age z_score value                   ##\n",
        "##        4. yolo_crop_img : Cropping Img Using YoloV5                                      ##\n",
        "##        5. show_img : Print Result Img                                                    ##\n",
        "##        6. out_crop_img : Declaration Variable of crop images                             ##                     \n",
        "##                                                                                          ##\n",
        "##                                    * Notes *                                             ##\n",
        "##        1. Several Time will be spent on loading Yolo model and TJnet model.              ##\n",
        "##        2. BA_mean value = 115.41626213592232                                             ##\n",
        "##        3. BA_std value = 48.02950411666953                                               ##\n",
        "##        4. BA mean, std values are Calculated by train DataFrame                          ##\n",
        "##                                                                                          ##\n",
        "##############################################################################################\n",
        "\n",
        "    ## 1. input_info\n",
        "\n",
        "    def input_info(gender):\n",
        "        global img_path, model_path, tjnet_path\n",
        "\n",
        "        gender = gender\n",
        "        img_path = img_path\n",
        "        model_path = '/content/drive/MyDrive/2차 프로젝트 원본 데이터/yolov5'\n",
        "        tjnet_path = '/content/drive/MyDrive/2차 프로젝트 원본 데이터/TJM/tjnet24.h5'\n",
        "\n",
        "        return [gender, img_path, model_path, tjnet_path]\n",
        "\n",
        "    ## 2. Bone_extraction\n",
        "\n",
        "    def Bone_extraction(img_path):\n",
        "\n",
        "        # model import\n",
        "        import numpy as np\n",
        "        import cv2\n",
        "        import matplotlib.pyplot as plt\n",
        "        from sklearn.linear_model import LinearRegression\n",
        "        import glob\n",
        "        import math\n",
        "\n",
        "        ## 2-1. read_img\n",
        "        def read_img(path):\n",
        "            original_img = cv2.imread(path)\n",
        "            return original_img\n",
        "\n",
        "        ## 2-2. make_mask\n",
        "        def make_mask(original_img):\n",
        "            # change to lab for making mask\n",
        "            img_mask = original_img.copy()\n",
        "            img_mask = cv2.cvtColor(img_mask, cv2.COLOR_RGB2BGR)\n",
        "            img_mask = cv2.cvtColor(img_mask, cv2.COLOR_BGR2Lab)\n",
        "            ## blur _02 \n",
        "            # kernel_size = odds / value = img.mean()\n",
        "            blur_k = int((img_mask.mean()*0.5)//2)*2+1\n",
        "            img_mask = cv2.medianBlur(img_mask, blur_k)\n",
        "            ## change to Grayscale for threshold\n",
        "            img_mask = cv2.cvtColor(img_mask, cv2.COLOR_Lab2BGR)\n",
        "            img_mask = cv2.cvtColor(img_mask, cv2.COLOR_BGR2GRAY)\n",
        "            ## binary / value = img.mean()\n",
        "            if img_mask.mean() > 100 : \n",
        "                th = img_mask.mean()*0.94\n",
        "            else : \n",
        "                th = img_mask.mean()\n",
        "            ret, img_mask = cv2.threshold(img_mask, th, 255, cv2.THRESH_BINARY)\n",
        "            ## mask based Max value of contours\n",
        "            contours, hierarchy = cv2.findContours(img_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "            max_cnt = max(contours, key=cv2.contourArea)\n",
        "            mask = np.zeros(img_mask.shape, dtype=np.uint8)\n",
        "            cv2.drawContours(mask, [max_cnt], -1, (255,255,255), -1)\n",
        "            ## Applying for dilation\n",
        "            k = cv2.getStructuringElement(cv2.MORPH_RECT, (8,8))\n",
        "            mask = cv2.dilate(mask,k)\n",
        "            return mask\n",
        "\n",
        "        ## 2-3. cut_mask\n",
        "        def cut_mask(original_img, mask):\n",
        "            ## copying\n",
        "            img_for_cut = original_img.copy()\n",
        "            ## H/W\n",
        "            height, width = img_for_cut.shape[:2]\n",
        "            ## mask\n",
        "            mask_list = mask.tolist()    \n",
        "            for y in range(int(height*0.05),height):\n",
        "                if max(mask[y,int(width*0.3):int(width*0.7)]) > 0:\n",
        "                    start_y = y-int(height*0.05)\n",
        "                    break            \n",
        "            for x in range(int(width*0.05),width):\n",
        "                if max(mask[int(height*0.3):int(height*0.7),x]) > 0:\n",
        "                    start_x = x-int(width*0.05)\n",
        "                    break           \n",
        "            for x in range(int(width*0.95),-1,-1):\n",
        "                if max(mask[int(height*0.3):int(height*0.7),x]) > 0:\n",
        "                    end_x = x+int(width*0.05)\n",
        "                    break               \n",
        "            cut_index = 0\n",
        "            if mask_list[height-1][-1] == 255 or mask_list[height-1][0] == 255:\n",
        "                for n in reversed(range(height)):\n",
        "                    if mask_list[n][0] == 0 or mask_list[n][-1] == 0:\n",
        "                        cut_index = n\n",
        "                        break            \n",
        "            if cut_index == 0:\n",
        "                cut_index = height\n",
        "            ## converting color\n",
        "            img_for_cut = cv2.cvtColor(img_for_cut, cv2.COLOR_BGR2GRAY) \n",
        "            img_for_cut = img_for_cut[start_y:(cut_index-1),start_x:end_x]\n",
        "            mask = mask[start_y:(cut_index-1),start_x:end_x]\n",
        "            ## remove background\n",
        "            masked = cv2.bitwise_and(img_for_cut, mask)\n",
        "            return masked\n",
        "        \n",
        "        ## 2-4. overlay_contours\n",
        "        def overlay_contours(masked):\n",
        "            copied_img = masked.copy()\n",
        "            ## smoothing\n",
        "            gray_smooth = cv2.bilateralFilter(copied_img, 7,60,60)\n",
        "            ## contours\n",
        "            outline = cv2.Canny(gray_smooth,50,50)\n",
        "            ## overlay\n",
        "            overlaied_img = cv2.add(masked,outline)\n",
        "            return overlaied_img\n",
        "\n",
        "        ## 2-5. img_rotation\n",
        "        def img_rotation(overlaied_img):\n",
        "            ## copying img\n",
        "            before_rot_img = overlaied_img.copy()\n",
        "            h, w = before_rot_img.shape[:2]\n",
        "            before_rot_img = cv2.cvtColor(before_rot_img, cv2.COLOR_RGB2BGR)\n",
        "            gray = cv2.cvtColor(before_rot_img, cv2.COLOR_BGR2GRAY)\n",
        "            ret, th = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
        "            th_li = th.tolist()\n",
        "            ## Rotation stage 01\n",
        "            # lower = first black spot\n",
        "            for i in reversed(range(h)):\n",
        "                if th_li[i][0] == 0 and th_li[i][-1] == 0:\n",
        "                    lower = i\n",
        "                    break\n",
        "            # lower = condition ; bottom = lower / img * 0.95\n",
        "            if lower == h - 1:\n",
        "                lower = int(h*0.9)\n",
        "            # upper = condition ; lower + lower * 0.05\n",
        "            slice5 = int(len(th)*0.05)\n",
        "            upper = lower - slice5\n",
        "            # x, y = between upper and lower (5%) / wrist center\n",
        "            x,y = [],[]\n",
        "            for i in range(slice5):\n",
        "                cnt = th_li[i + upper].count(255)\n",
        "                index = th_li[i + upper].index(255)\n",
        "                x.append([i+upper])\n",
        "                y.append([int((index*2 + cnt - 1)/2)])\n",
        "            # x, y / draw regression line\n",
        "            model = LinearRegression()\n",
        "            model.fit(X=x,y=y)\n",
        "            ####################################################\n",
        "            ## Rotation stage 02        \n",
        "            angle = math.atan2(h - 0, int(model.predict([[h]])) - int(model.predict([[0]])))*180/math.pi\n",
        "            M = cv2.getRotationMatrix2D((w/2,h/2), angle-90, 1)\n",
        "            rotate = cv2.warpAffine(before_rot_img, M, (w, h))\n",
        "            # Cutting img (rotated img)\n",
        "            for i in range(len(th[-1])):\n",
        "                if th[-1][i] == 255:\n",
        "                    start_x = i\n",
        "                    break\n",
        "            for i in range(len(th[-1])):\n",
        "                if th[-1][i] == 255:\n",
        "                    end_x = i                   \n",
        "            s_point = h - int((int(model.predict([[h]])-start_x)) * math.tan(math.pi*((90-angle)/180)))\n",
        "            e_point = h - int((end_x - int(model.predict([[h]]))) * math.tan(math.pi*((angle-90)/180)))\n",
        "            point = max(s_point, e_point)\n",
        "            rotated_img = rotate[:point]\n",
        "            return rotated_img\n",
        "\n",
        "        ## 2-6. contrast_roi\n",
        "        def contrast_roi(img, low, high):\n",
        "            ## height, width\n",
        "            h, w = img.shape\n",
        "            img_ = np.zeros(img.shape, dtype=np.uint8)\n",
        "            for y in range(h):\n",
        "                for x in range(w):\n",
        "                    temp = int((255 / (high - low)) * (img[y][x] - low))\n",
        "                    if temp > 255:\n",
        "                        img_[y][x] = 255\n",
        "                    elif temp < 0:\n",
        "                        img_[y][x] = 0\n",
        "                    else:\n",
        "                        img_[y][x] = temp\n",
        "            return img_\n",
        "\n",
        "        ## 2-7. bright_ness\n",
        "        def bright_ness(img):\n",
        "            ## columns, rows\n",
        "            cols, rows = img.shape[:2]\n",
        "            brightness = np.sum(img) / (255 * cols * rows)\n",
        "            return brightness\n",
        "\n",
        "        ## 2-8. Decomposing\n",
        "        def Decomposing(rotated_img,a,b,d,e):\n",
        "            ######## Decomposing_stage_1 / [ Contours , Mask ] ########\n",
        "            decomp_img_1 = rotated_img.copy()\n",
        "            ## Adjusting brighness\n",
        "            if bright_ness(decomp_img_1) > 0.8:\n",
        "                decomp_img_1 = np.clip(decomp_img_1 - 80., 0, 255).astype(np.uint8)\n",
        "            elif bright_ness(decomp_img_1) > 0.75:\n",
        "                decomp_img_1 = np.clip(decomp_img_1 - 50., 0, 255).astype(np.uint8)\n",
        "            elif bright_ness(decomp_img_1) > 0.65:\n",
        "                decomp_img_1 = np.clip(decomp_img_1 - 30., 0, 255).astype(np.uint8)\n",
        "            else: decomp_img_1 = np.clip(decomp_img_1 - 10., 0, 255).astype(np.uint8)\n",
        "            ## change to Lab\n",
        "            decomp_img_1 = cv2.cvtColor(decomp_img_1, cv2.COLOR_RGB2BGR)\n",
        "            decomp_img_1 = cv2.cvtColor(decomp_img_1, cv2.COLOR_BGR2Lab)\n",
        "            ## Morphology\n",
        "            k = cv2.getStructuringElement(cv2.MORPH_CROSS, (a, a))\n",
        "            decomp_img_1 = cv2.morphologyEx(decomp_img_1, cv2.MORPH_TOPHAT, k) # Emphasis\n",
        "            ## Filter\n",
        "            decomp_img_1 = cv2.bilateralFilter(decomp_img_1,-1, d, e)\n",
        "            ## Lab to gray for binary\n",
        "            decomp_img_1 = cv2.cvtColor(decomp_img_1, cv2.COLOR_Lab2BGR)\n",
        "            decomp_img_1 = cv2.cvtColor(decomp_img_1, cv2.COLOR_BGR2GRAY)\n",
        "            ## img_normalization\n",
        "            decomp_img_1 = cv2.normalize(decomp_img_1, None, 0, 255, cv2.NORM_MINMAX)\n",
        "            ## CLAHE\n",
        "            decomp_img_1 = cv2.equalizeHist(decomp_img_1)\n",
        "            clahe = cv2.createCLAHE(clipLimit=1.0, tileGridSize=(3,3)) \n",
        "            decomp_img_1= clahe.apply(decomp_img_1)          \n",
        "            ## Threshold / value = img.mean()\n",
        "            ret, mask = cv2.threshold(decomp_img_1,\n",
        "                                    np.mean(decomp_img_1),\n",
        "                                    255,\n",
        "                                    cv2.THRESH_BINARY) \n",
        "            ## Extract object / same value pixels\n",
        "            contours, hierarchy = cv2.findContours(mask, \n",
        "                                                    cv2.RETR_EXTERNAL, # only outline\n",
        "                                                    cv2.CHAIN_APPROX_SIMPLE) # Contour vertex coordinate\n",
        "            ## drawing Contours\n",
        "            cv2.drawContours(mask, contours, -1, (255,255,255), -1) # -1: 모든 컨트어 표시 /color/ fill             \n",
        "            ######## Decomposing_stage_2 / [ Brightness_Empahsis ] ########\n",
        "            ## Empahsis\n",
        "            decomp_img_2 = rotated_img.copy()\n",
        "            if bright_ness(decomp_img_2) > 0.8:\n",
        "                decomp_img_2 = np.clip(decomp_img_2 - 80., 0, 255).astype(np.uint8)\n",
        "            elif bright_ness(decomp_img_2) > 0.75:\n",
        "                decomp_img_2 = np.clip(decomp_img_2 - 50., 0, 255).astype(np.uint8)\n",
        "            elif bright_ness(decomp_img_2) > 0.65:\n",
        "                decomp_img_2 = np.clip(decomp_img_2 - 30., 0, 255).astype(np.uint8)\n",
        "            else: decomp_img_2 = np.clip(decomp_img_2 - 10., 0, 255).astype(np.uint8)\n",
        "            ## Morphology\n",
        "            k2 = cv2.getStructuringElement(cv2.MORPH_CROSS,(b,b))\n",
        "            decomp_img_2 = cv2.morphologyEx(decomp_img_2, cv2.MORPH_TOPHAT, k2)\n",
        "            ## contrast\n",
        "            decomp_img_2 = cv2.cvtColor(decomp_img_2, cv2.COLOR_BGR2RGB)\n",
        "            decomp_img_2 = cv2.cvtColor(decomp_img_2, cv2.COLOR_BGR2GRAY)\n",
        "            if decomp_img_2.mean() <= 15:\n",
        "                low = decomp_img_2.mean() * 3.2\n",
        "                high = decomp_img_2.mean() * 3.6\n",
        "            elif decomp_img_2.mean() <= 20:\n",
        "                low = decomp_img_2.mean() * 3\n",
        "                high = decomp_img_2.mean() * 3.6\n",
        "            else:\n",
        "                low = decomp_img_2.mean() * 3\n",
        "                high = decomp_img_2.mean() * 3.7\n",
        "            decomp_img_2 = cv2.blur(decomp_img_2,(2,2))\n",
        "            decomp_img_2 = contrast_roi(decomp_img_2, low, high)\n",
        "            ######## Decomposing_Final_stage / [ Result ] ########\n",
        "            ### Bone empahsis / bitwise (mask)\n",
        "            ## Morphology\n",
        "            ## Contours\n",
        "            contours, hierarchy = cv2.findContours(decomp_img_2, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "            cv2.drawContours(decomp_img_2, contours, -1, (255, 255, 255), -1)\n",
        "            ## Bitwise (mask) / print white parts\n",
        "            decomp_img_2 = cv2.bitwise_and(decomp_img_2, mask) \n",
        "            decomp_img_2 = cv2.cvtColor(decomp_img_2, cv2.COLOR_GRAY2BGR)\n",
        "            decomp_img_2 = cv2.blur(decomp_img_2,(2,2))\n",
        "            bone_extraction = cv2.resize(decomp_img_2, (600, 800))\n",
        "            return bone_extraction\n",
        "\n",
        "        ## Bone_extraction (Running code)\n",
        "        try:\n",
        "            global bone\n",
        "            original_img = read_img(img_path)\n",
        "            mask = make_mask(original_img)\n",
        "            masked = cut_mask(original_img, mask)\n",
        "            # overlaied_img = overlay_contours(masked)\n",
        "            rotated_img = img_rotation(masked)\n",
        "            bone = Decomposing(rotated_img,60,55,50,25)\n",
        "            \n",
        "            # Save extraction img\n",
        "            global save_path\n",
        "            import re\n",
        "            regex = re.compile('[^.+]+')\n",
        "            test = regex.findall(img_path)\n",
        "            save_path = test[0] + '_extraction' + '.jpg'\n",
        "            cv2.imwrite(save_path, bone)\n",
        "            return bone, save_path\n",
        "\n",
        "        except:\n",
        "            print('ERROR > Please check again' )\n",
        "\n",
        "    ## 3. predict_zscore\n",
        "\n",
        "    def predict_zscore(gender,save_path, model_path,tjnet_path):\n",
        "\n",
        "        # Function connection line ; yolo_crop_img(4)\n",
        "        yolo_crop_img(gender, save_path, model_path)\n",
        "\n",
        "        import  numpy as np\n",
        "        import tensorflow.keras as tf\n",
        "\n",
        "        model = tf.models.load_model(tjnet_path, compile=False)\n",
        "\n",
        "        # grobal X : X_ray 이미지의 yolo crop image 값\n",
        "\n",
        "        y_predict = model.predict(X)\n",
        "        global pred\n",
        "        pred = y_predict[0][0] \n",
        "\n",
        "    ## 4. yolo_crop_img\n",
        "\n",
        "\n",
        "    def yolo_crop_img(gender, save_path,model_path):\n",
        "        import os\n",
        "        import numpy as np\n",
        "        global result\n",
        "        os.chdir(model_path)\n",
        "        import torch\n",
        "    \n",
        "        model = torch.load('./model.pt', map_location='cpu')\n",
        "        result = model(save_path)\n",
        "        global crops\n",
        "        crops = result.crop(save=False)\n",
        "        global X\n",
        "        X = out_crop_img(crops, gender)\n",
        "        global img\n",
        "        img = np.squeeze(result.render())\n",
        "        return result\n",
        "\n",
        "\n",
        "    ## 5. show_img\n",
        "    global show_img\n",
        "    def show_img(result):\n",
        "        import matplotlib.pyplot as plt\n",
        "        import numpy as np\n",
        "        %matplotlib inline\n",
        "        plt.figure(figsize=(16,12))\n",
        "        plt.imshow(np.squeeze(result.render()))\n",
        "        plt.show()\n",
        "\n",
        "    ## 6. out_crop_img\n",
        "\n",
        "    def out_crop_img(crop, gender):\n",
        "        import re\n",
        "        import cv2\n",
        "        import numpy as np\n",
        "\n",
        "        gender = np.array(gender).reshape(1,1)\n",
        "\n",
        "        for i in range(7):\n",
        "            carpal = re.compile('CARPAL.')\n",
        "            ip = re.compile('IP.')\n",
        "            lmcp = re.compile('LMCP.')\n",
        "            lpip = re.compile('LPIP.')\n",
        "            mmcp = re.compile('MMCP.')\n",
        "            mpip = re.compile('MPIP.')\n",
        "            tmcp = re.compile('TMCP.')\n",
        "\n",
        "            if carpal.search(crop[i]['label']):\n",
        "                CARPAL_img = crop[i]['im']\n",
        "                CARPAL_img = cv2.resize(CARPAL_img, (224,224),cv2.INTER_AREA)\n",
        "                CARPAL_img = np.expand_dims(CARPAL_img, axis=0)\n",
        "\n",
        "            if ip.search(crop[i]['label']):\n",
        "                IP_img = crop[i]['im']\n",
        "                IP_img = cv2.resize(IP_img, (75,75),cv2.INTER_AREA)\n",
        "                IP_img = np.expand_dims(IP_img, axis=0)\n",
        "                \n",
        "            if lmcp.search(crop[i]['label']):\n",
        "                LMCP_img = crop[i]['im']\n",
        "                LMCP_img = cv2.resize(LMCP_img, (75,75),cv2.INTER_AREA)\n",
        "                LMCP_img = np.expand_dims(LMCP_img, axis=0)\n",
        "\n",
        "            if lpip.search(crop[i]['label']):\n",
        "                LPIP_img = crop[i]['im']\n",
        "                LPIP_img = cv2.resize(LPIP_img, (75,75),cv2.INTER_AREA)\n",
        "                LPIP_img = np.expand_dims(LPIP_img, axis=0)\n",
        "            \n",
        "            if mmcp.search(crop[i]['label']):\n",
        "                MMCP_img = crop[i]['im']\n",
        "                MMCP_img = cv2.resize(MMCP_img, (75,75),cv2.INTER_AREA)\n",
        "                MMCP_img = np.expand_dims(MMCP_img, axis=0)\n",
        "                \n",
        "            if mpip.search(crop[i]['label']):\n",
        "                MPIP_img = crop[i]['im']\n",
        "                MPIP_img = cv2.resize(MPIP_img, (75,75),cv2.INTER_AREA)\n",
        "                MPIP_img = np.expand_dims(MPIP_img, axis=0)\n",
        "\n",
        "            if tmcp.search(crop[i]['label']):\n",
        "                TMCP_img = crop[i]['im']\n",
        "                TMCP_img = cv2.resize(TMCP_img, (75,75),cv2.INTER_AREA)\n",
        "                TMCP_img = np.expand_dims(TMCP_img, axis=0)\n",
        "\n",
        "            else : continue\n",
        "\n",
        "        return [CARPAL_img, LMCP_img, MMCP_img,TMCP_img, LPIP_img, MPIP_img, IP_img, gender]\n",
        "\n",
        "#### All Functions line ####\n",
        "\n",
        "    gender, img_path, model_path, tjnet_path = input_info(gender)\n",
        "    bone, save_path = Bone_extraction(img_path)\n",
        "    predict_zscore(gender, save_path, model_path,tjnet_path) #return pred\n",
        "    BA_mean = 115.41626213592232                                            \n",
        "    BA_std = 48.02950411666953         \n",
        "    global prediction_BA\n",
        "    prediction_BA = (pred * BA_std + BA_mean)/12\n",
        "    return prediction_BA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFhDzKWJB1VM"
      },
      "source": [
        "### GUI Code _ Test code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ywxiKeaAC9J"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFgNdYvMowVf"
      },
      "outputs": [],
      "source": [
        "img_path ='/content/drive/MyDrive/2차 프로젝트 원본 데이터/데이터 원본/image/Female/1_F.jpg'\n",
        "gender = 0\n",
        "Total_pred_gui(gender)\n",
        "print('prediction_BA > {0} Year'.format( round(prediction_BA,2)))\n",
        "show_img(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkVi9OhcpiS8"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "## For Loop\n",
        "import pandas as pd\n",
        "import re\n",
        "regex = re.compile('[^.+]+')\n",
        "regex2 = re.compile(r'\\d+')\n",
        "\n",
        "# A = '/content/drive/MyDrive/2차 프로젝트 원본 데이터/Code_test_data/015_M.jpg'\n",
        "# B = '/content/drive/MyDrive/2차 프로젝트 원본 데이터/Code_test_data/252_F.jpg'\n",
        "# C = '/content/drive/MyDrive/2차 프로젝트 원본 데이터/Code_test_data/663_M.jpg'\n",
        "# D = '/content/drive/MyDrive/2차 프로젝트 원본 데이터/Code_test_data/401_F.jpg'\n",
        "\n",
        "# path_list = [A,B,C,D]\n",
        "path_list = '/content/drive/MyDrive/2차 프로젝트 원본 데이터/B_A_img_data/490_F.jpg',\n",
        "\n",
        "\n",
        "df = pd.read_pickle('/content/drive/MyDrive/2차 프로젝트 원본 데이터/BA_all_.pkl')\n",
        "\n",
        "from tqdm import tqdm\n",
        "for path in tqdm(path_list):\n",
        "\n",
        "    if '_F' in path:\n",
        "        gender = 0\n",
        "    elif '_M' in path:\n",
        "        gender = 1\n",
        "\n",
        "    img_path = path \n",
        "\n",
        "    df = pd.read_pickle('/content/drive/MyDrive/2차 프로젝트 원본 데이터/BA_all_.pkl')    \n",
        "    test = regex.findall(path)\n",
        "    text = regex2.findall(test[0]) \n",
        "    A = df[ (df['NO']==int(text[1])) & (df['GENDER']==gender) ][['NO','BA_1','BA_2','BA_TOTAL']]\n",
        "\n",
        "    print()\n",
        "    print('gender > ' , gender)\n",
        "    display('Real_age_in_df > ', A)\n",
        "    Total_pred_gui(gender)\n",
        "    print('prediction_BA > {0} Year'.format( round(prediction_BA,2)))\n",
        "    show_img(result)   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7V1RsUZrEpl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "Final_Auto_code.ipynb의 사본",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
